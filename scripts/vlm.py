# scripts/vlm.py

import os
import time
import json
import base64
import warnings
from pathlib import Path
from huggingface_hub import InferenceClient

# â”€â”€â”€ 1) Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
warnings.filterwarnings("ignore")
HF_TOKEN = os.getenv("HF_TOKEN")
if not HF_TOKEN:
    raise RuntimeError("Please set the HF_TOKEN environment variable (your HF API key)")

# Choose the provider & model for your visionâ€language chat
CLIENT = InferenceClient(provider="nebius", api_key=HF_TOKEN, timeout=30)
MODEL_ID = "Qwen/Qwen2.5-VL-72B-Instruct"

SYSTEM_INSTRUCTIONS = (
    "You are a friendly, patient, and highly accurate medical assistant. "
    "Your task is to answer the userâ€™s question strictly based on the contents of the provided medication label image. "
    "Follow these rules:\n"
    "  â€¢ Only use information actually visible on the labelâ€”do NOT hallucinate or invent details.\n"
    "  â€¢ Be clear and concise, in simple everyday language.\n"
    "  â€¢ If the label does not show the answer, reply: "
    "â€œIâ€™m not sure from this label alone; please check with a pharmacist or doctor.â€\n"
    "  â€¢ Provide dosage, usage, safety, or storage details exactly as written.\n"
    "  â€¢ Keep your answer polite and patientâ€‘focused."
)

# â”€â”€â”€ 2) Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _image_to_data_url(image_path: str) -> str:
    """Embed a local JPG/PNG into a base64 data URL for the API."""
    with open(image_path, "rb") as img_f:
        b64 = base64.b64encode(img_f.read()).decode("utf-8")
    ext = Path(image_path).suffix.lstrip(".").lower()
    return f"data:image/{ext};base64,{b64}"

# â”€â”€â”€ 3) Core Vâ€‘L function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def generate_answer_via_chat(image_path: str, user_question: str) -> str:
    """
    Sends a vision+text prompt to the Vâ€‘L chat model and returns its reply.

    Args:
        image_path: local path to the medicationâ€label image
        user_question: the question string to ask about that label

    Returns:
        The modelâ€™s plaintext answer.
    """
    # 1) Encode the image
    image_data = _image_to_data_url(image_path)

    # 2) Build the two messages
    system_msg = {"role": "system", "content": SYSTEM_INSTRUCTIONS}
    user_msg = {
        "role": "user",
        "content": [
            {"type": "text", "text": f"Question about this medication label: {user_question}"},
            {"type": "image_url", "image_url": {"url": image_data}}
        ]
    }

    # 3) Call the API
    start = time.time()
    completion = CLIENT.chat.completions.create(
        model=MODEL_ID,
        messages=[system_msg, user_msg],
        max_tokens=256,
        temperature=0.0  # deterministic
    )
    elapsed = time.time() - start

    # 4) Extract & return
    if not completion.choices:
        raise RuntimeError("Empty response from Vâ€‘L model")
    answer = completion.choices[0].message.content.strip()

    # (optional) print timing
    print(f"ğŸ–¼â†’ğŸ’¬ Vâ€‘L call done in {elapsed:.1f}s")
    return answer
